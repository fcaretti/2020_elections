{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of supporting Biden or Trump in the 2020 American election\n",
    "Our goal is to understand which words are the most important for both voting groups. We will train a Naive Bayes classifier to classify the two groups and then evaluate the performance of the classifier on a validation set.\n",
    "Then, we will understand which words have the highest (or lowest) posteriors, in order to understand which topics are the most relevant between the two voting basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/federico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor as pre\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(10) #it will be useful to have the same dictionary every time, in order to write it only once\n",
    "import re                                  \n",
    "import string                              \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "import pickle\n",
    "from os import getcwd\n",
    "import json\n",
    "import elections_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df = pd.read_csv(r\"./hashtag_donaldtrump.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_df = pd.read_csv(r\"./hashtag_joebiden.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(970919, 21)\n",
      "(776886, 21)\n"
     ]
    }
   ],
   "source": [
    "print(trump_df.shape)\n",
    "print(biden_df.shape)\n",
    "#the two datasets are not perfectly balanced, but probably sufficiently enough for NB to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 970919 entries, 0 to 970918\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   created_at            970919 non-null  object \n",
      " 1   tweet_id              970919 non-null  float64\n",
      " 2   tweet                 970919 non-null  object \n",
      " 3   likes                 970919 non-null  float64\n",
      " 4   retweet_count         970919 non-null  float64\n",
      " 5   source                970043 non-null  object \n",
      " 6   user_id               970919 non-null  float64\n",
      " 7   user_name             970903 non-null  object \n",
      " 8   user_screen_name      970919 non-null  object \n",
      " 9   user_description      869653 non-null  object \n",
      " 10  user_join_date        970919 non-null  object \n",
      " 11  user_followers_count  970919 non-null  float64\n",
      " 12  user_location         675966 non-null  object \n",
      " 13  lat                   445719 non-null  float64\n",
      " 14  long                  445719 non-null  float64\n",
      " 15  city                  227187 non-null  object \n",
      " 16  country               442748 non-null  object \n",
      " 17  continent             442765 non-null  object \n",
      " 18  state                 320620 non-null  object \n",
      " 19  state_code            300425 non-null  object \n",
      " 20  collected_at          970919 non-null  object \n",
      "dtypes: float64(7), object(14)\n",
      "memory usage: 155.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 776886 entries, 0 to 776885\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   created_at            776886 non-null  object \n",
      " 1   tweet_id              776886 non-null  float64\n",
      " 2   tweet                 776886 non-null  object \n",
      " 3   likes                 776886 non-null  float64\n",
      " 4   retweet_count         776886 non-null  float64\n",
      " 5   source                776173 non-null  object \n",
      " 6   user_id               776886 non-null  float64\n",
      " 7   user_name             776868 non-null  object \n",
      " 8   user_screen_name      776886 non-null  object \n",
      " 9   user_description      694880 non-null  object \n",
      " 10  user_join_date        776886 non-null  object \n",
      " 11  user_followers_count  776886 non-null  float64\n",
      " 12  user_location         543095 non-null  object \n",
      " 13  lat                   355293 non-null  float64\n",
      " 14  long                  355293 non-null  float64\n",
      " 15  city                  186872 non-null  object \n",
      " 16  country               353779 non-null  object \n",
      " 17  continent             353797 non-null  object \n",
      " 18  state                 260195 non-null  object \n",
      " 19  state_code            244609 non-null  object \n",
      " 20  collected_at          776886 non-null  object \n",
      "dtypes: float64(7), object(14)\n",
      "memory usage: 124.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(trump_df.info())\n",
    "print(biden_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this small project, we will only be interested in the tweets themselves, although many other interesting pieces of information may be retrieved from the dataset, such as the approval rate of the candidates inside vs outside the US or the amount of tweets from specific states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add a column for each candidate\n",
    "#this is necessary to do supervised learning, since we will later remove the names of the candidates from the tweets\n",
    "trump_df[\"Candidate\"] = 'trump'\n",
    "biden_df[\"Candidate\"] = 'biden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_description</th>\n",
       "      <th>...</th>\n",
       "      <th>user_location</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>state</th>\n",
       "      <th>state_code</th>\n",
       "      <th>collected_at</th>\n",
       "      <th>Candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-15 00:00:01</td>\n",
       "      <td>1.316529e+18</td>\n",
       "      <td>#Elecciones2020 | En #Florida: #JoeBiden dice ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>3.606665e+08</td>\n",
       "      <td>El Sol Latino News</td>\n",
       "      <td>elsollatinonews</td>\n",
       "      <td>üåê Noticias de inter√©s para latinos de la costa...</td>\n",
       "      <td>...</td>\n",
       "      <td>Philadelphia, PA / Miami, FL</td>\n",
       "      <td>25.774270</td>\n",
       "      <td>-80.193660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>North America</td>\n",
       "      <td>Florida</td>\n",
       "      <td>FL</td>\n",
       "      <td>2020-10-21 00:00:00</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-15 00:00:01</td>\n",
       "      <td>1.316529e+18</td>\n",
       "      <td>Usa 2020, Trump contro Facebook e Twitter: cop...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Social Mediaset</td>\n",
       "      <td>3.316176e+08</td>\n",
       "      <td>Tgcom24</td>\n",
       "      <td>MediasetTgcom24</td>\n",
       "      <td>Profilo ufficiale di Tgcom24: tutte le notizie...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-10-21 00:00:00.373216530</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-15 00:00:02</td>\n",
       "      <td>1.316529e+18</td>\n",
       "      <td>#Trump: As a student I used to hear for years,...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>8.436472e+06</td>\n",
       "      <td>snarke</td>\n",
       "      <td>snarke</td>\n",
       "      <td>Will mock for food! Freelance writer, blogger,...</td>\n",
       "      <td>...</td>\n",
       "      <td>Portland</td>\n",
       "      <td>45.520247</td>\n",
       "      <td>-122.674195</td>\n",
       "      <td>Portland</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>North America</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>OR</td>\n",
       "      <td>2020-10-21 00:00:00.746433060</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-15 00:00:02</td>\n",
       "      <td>1.316529e+18</td>\n",
       "      <td>2 hours since last tweet from #Trump! Maybe he...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trumpytweeter</td>\n",
       "      <td>8.283556e+17</td>\n",
       "      <td>Trumpytweeter</td>\n",
       "      <td>trumpytweeter</td>\n",
       "      <td>If he doesn't tweet for some time, should we b...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-10-21 00:00:01.119649591</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-15 00:00:08</td>\n",
       "      <td>1.316529e+18</td>\n",
       "      <td>You get a tie! And you get a tie! #Trump ‚Äòs ra...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>4.741380e+07</td>\n",
       "      <td>Rana Abtar - ÿ±ŸÜÿß ÿ£ÿ®ÿ™ÿ±</td>\n",
       "      <td>Ranaabtar</td>\n",
       "      <td>Washington Correspondent, Lebanese-American ,c...</td>\n",
       "      <td>...</td>\n",
       "      <td>Washington DC</td>\n",
       "      <td>38.894992</td>\n",
       "      <td>-77.036558</td>\n",
       "      <td>Washington</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>North America</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>DC</td>\n",
       "      <td>2020-10-21 00:00:01.492866121</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at      tweet_id  \\\n",
       "0  2020-10-15 00:00:01  1.316529e+18   \n",
       "1  2020-10-15 00:00:01  1.316529e+18   \n",
       "2  2020-10-15 00:00:02  1.316529e+18   \n",
       "3  2020-10-15 00:00:02  1.316529e+18   \n",
       "4  2020-10-15 00:00:08  1.316529e+18   \n",
       "\n",
       "                                               tweet  likes  retweet_count  \\\n",
       "0  #Elecciones2020 | En #Florida: #JoeBiden dice ...    0.0            0.0   \n",
       "1  Usa 2020, Trump contro Facebook e Twitter: cop...   26.0            9.0   \n",
       "2  #Trump: As a student I used to hear for years,...    2.0            1.0   \n",
       "3  2 hours since last tweet from #Trump! Maybe he...    0.0            0.0   \n",
       "4  You get a tie! And you get a tie! #Trump ‚Äòs ra...    4.0            3.0   \n",
       "\n",
       "               source       user_id              user_name user_screen_name  \\\n",
       "0           TweetDeck  3.606665e+08     El Sol Latino News  elsollatinonews   \n",
       "1    Social Mediaset   3.316176e+08                Tgcom24  MediasetTgcom24   \n",
       "2     Twitter Web App  8.436472e+06                 snarke           snarke   \n",
       "3       Trumpytweeter  8.283556e+17          Trumpytweeter    trumpytweeter   \n",
       "4  Twitter for iPhone  4.741380e+07  Rana Abtar - ÿ±ŸÜÿß ÿ£ÿ®ÿ™ÿ±        Ranaabtar   \n",
       "\n",
       "                                    user_description  ...  \\\n",
       "0  üåê Noticias de inter√©s para latinos de la costa...  ...   \n",
       "1  Profilo ufficiale di Tgcom24: tutte le notizie...  ...   \n",
       "2  Will mock for food! Freelance writer, blogger,...  ...   \n",
       "3  If he doesn't tweet for some time, should we b...  ...   \n",
       "4  Washington Correspondent, Lebanese-American ,c...  ...   \n",
       "\n",
       "                  user_location        lat        long        city  \\\n",
       "0  Philadelphia, PA / Miami, FL  25.774270  -80.193660         NaN   \n",
       "1                           NaN        NaN         NaN         NaN   \n",
       "2                      Portland  45.520247 -122.674195    Portland   \n",
       "3                           NaN        NaN         NaN         NaN   \n",
       "4                 Washington DC  38.894992  -77.036558  Washington   \n",
       "\n",
       "                    country      continent                 state state_code  \\\n",
       "0  United States of America  North America               Florida         FL   \n",
       "1                       NaN            NaN                   NaN        NaN   \n",
       "2  United States of America  North America                Oregon         OR   \n",
       "3                       NaN            NaN                   NaN        NaN   \n",
       "4  United States of America  North America  District of Columbia         DC   \n",
       "\n",
       "                    collected_at Candidate  \n",
       "0            2020-10-21 00:00:00     trump  \n",
       "1  2020-10-21 00:00:00.373216530     trump  \n",
       "2  2020-10-21 00:00:00.746433060     trump  \n",
       "3  2020-10-21 00:00:01.119649591     trump  \n",
       "4  2020-10-21 00:00:01.492866121     trump  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mix the data together\n",
    "Data_Mixed = pd.concat([trump_df,biden_df])\n",
    "Data_Mixed.sort_values(by='created_at')\n",
    "Data_Mixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#note that the number of likes for pro-biden tweets is higher than the ones from trump supporters, although\n",
    "#the total number of tweets pro-trump is higher\n",
    "Data_Mixed.groupby('Candidate')['likes'].sum().plot.bar()\n",
    "plt.ylabel('Number of Likes')\n",
    "plt.title('Trump vs Biden')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of likes of pro-trump tweets:7.4770109555997974\n",
      "Average number of likes of pro-biden tweets:10.161163928813236\n"
     ]
    }
   ],
   "source": [
    "#average number of likes for a pro-biden tweet vs a pro-trump tweet\n",
    "\n",
    "print(f'Average number of likes of pro-trump tweets:'+ \n",
    "      str((Data_Mixed.loc[Data_Mixed['Candidate']=='trump']['likes'].sum())/float(trump_df.shape[0])))\n",
    "print(f'Average number of likes of pro-biden tweets:'+ \n",
    "      str((Data_Mixed.loc[Data_Mixed['Candidate']=='biden']['likes'].sum())/float(biden_df.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Elecciones2020 | En #Florida: #JoeBiden dice que #DonaldTrump solo se preocupa por √©l mismo. El dem√≥crata fue anfitri√≥n de encuentros de electores en #PembrokePines y #Miramar. Clic AQU√ç ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\n",
      "‚†Ä\n",
      "üåêhttps://t.co/qhIWpIUXsT\n",
      "_\n",
      "#ElSolLatino #yobrilloconelsol https://t.co/6FlCBWf1Mi\n",
      "178899\n"
     ]
    }
   ],
   "source": [
    "trump_tweets=trump_df.loc[trump_df['country']=='United States of America']['tweet'].tolist()\n",
    "#notice that this will be considered a pro-trump tweet, since it has the #Trump.. that's normal data noise\n",
    "print(trump_tweets[0])\n",
    "print(len(trump_tweets))\n",
    "#in case our classifier gets a high misclassification rate, then we might infer that tweets that talk about trump\n",
    "#or biden use more or less the same words\n",
    "#in that case, we may try to do a sentiment analysis for the two datasets separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Elecciones2020 | En #Florida: #JoeBiden dice que #DonaldTrump solo se preocupa por √©l mismo. El dem√≥crata fue anfitri√≥n de encuentros de electores en #PembrokePines y #Miramar. Clic AQU√ç ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\n",
      "‚†Ä\n",
      "üåêhttps://t.co/qhIWpIUXsT\n",
      "_\n",
      "#ElSolLatino #yobrilloconelsol https://t.co/6FlCBWf1Mi\n",
      "153596\n"
     ]
    }
   ],
   "source": [
    "biden_tweets=biden_df.loc[biden_df['country']=='United States of America']['tweet'].tolist()\n",
    "print(biden_tweets[0])\n",
    "print(len(biden_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above tweets are the same, for both candidates! We will remove them, in order to hopefully remove some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=False\n",
    "try:\n",
    "    with open('./trump_tweets') as f:\n",
    "        file=True\n",
    "except IOError:\n",
    "    print(\"File not accessible\")\n",
    "\n",
    "if file==False:\n",
    "    random.shuffle(biden_tweets)\n",
    "    random.shuffle(trump_tweets)\n",
    "    print(len(trump_tweets))\n",
    "    i=0\n",
    "    for tweet in trump_tweets:\n",
    "        i=i+1\n",
    "        if('joe' or 'Joe' or 'JOE' or 'biden' or 'Biden' or 'BIDEN') in tweet:\n",
    "            trump_tweets.remove(tweet)\n",
    "    print(len(trump_tweets))\n",
    "    print('end trump')\n",
    "    i=0\n",
    "    print(len(biden_tweets))\n",
    "    for tweet in biden_tweets:\n",
    "        i=i+1\n",
    "        if('donald' or 'Donald' or 'DONALD' or 'trump' or 'Trump' or 'TRUMP') in tweet:\n",
    "            biden_tweets.remove(tweet)\n",
    "    print(len(biden_tweets))\n",
    "    with open(\"trump_tweets\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(trump_tweets, fp)\n",
    "    with open(\"biden_tweets\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(biden_tweets, fp)\n",
    "\n",
    "        \n",
    "if file==True:\n",
    "    with open(\"trump_tweets\", \"rb\") as fp:   # Unpickling\n",
    "        trump_tweets = pickle.load(fp)\n",
    "    with open(\"biden_tweets\", \"rb\") as fp:   # Unpickling\n",
    "        biden_tweets = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146132\n",
      "16237\n",
      "124530\n",
      "13837\n"
     ]
    }
   ],
   "source": [
    "#we use 10% of the tweets as test set\n",
    "trump_train=trump_tweets[:-round(len(trump_tweets)/10)]\n",
    "trump_test=trump_tweets[-round(len(trump_tweets)/10):]\n",
    "biden_train=biden_tweets[:-round(len(biden_tweets)/10)]\n",
    "biden_test=biden_tweets[-round(len(biden_tweets)/10):]\n",
    "print(len(trump_train))\n",
    "print(len(trump_test))\n",
    "print(len(biden_train))\n",
    "print(len(biden_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a custom function for tweet preprocessing; not only we need to remove the usual stuff (hyperlinks, hashtags, ecc) but also the names of the candidates\n",
    "\n",
    "You can find process_tweet and other helpful functions in the elections_utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['former', 'entrepreneur', 'hous', 'market']\n",
      "['politician', 'sinc', 'twenti']\n"
     ]
    }
   ],
   "source": [
    "print(elections_utils.process_tweet('@realDonaldTrump is a former entrepreneur in housing market'))\n",
    "print(elections_utils.process_tweet('Biden has been a politician since he was twenty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Trump-Biden Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from now on, we will use trump=1, biden=0\n",
    "x_train=trump_train+biden_train\n",
    "x_test=trump_test+biden_test\n",
    "y_train=np.append(np.ones(len(trump_train)), np.zeros(len(biden_train)))\n",
    "y_test=np.append(np.ones(len(trump_test)), np.zeros(len(biden_test)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the next function takes a dictionary and two lists (x and y) in input and outputs the dictionary\n",
    "#to the dictionary, new entries are added\n",
    "#the final dictionary has entries (word, 0/1)-->frequencies\n",
    "#for instance, if \"home\" occurs twice in Biden tweets, the dictionary will map (home,0) into 2\n",
    "try:\n",
    "    with open(\"./elections_dictionary\", \"rb\") as fp:   # Unpickling\n",
    "        freqs_dict = pickle.load(fp)\n",
    "except IOError:\n",
    "    freqs_dict = elections_utils.count_tweets({}, x_train, y_train)\n",
    "    with open(\"elections_dictionary\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(freqs_dict, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course NB is not really \"trained\", but the next function is called train_naive_bayse for semplicity.\n",
    "The function takes in input the dictionary with the frequencies and the two lists (of tweets and (0,1)).\n",
    "It returns the log prior (which is a number, since it is common to all words) and the log likelihood, which is a dictionary that maps every word into the corresponding log lokelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1599636721034231\n",
      "111211\n"
     ]
    }
   ],
   "source": [
    "#since evaluating the loglikelihood is quite long, and the seed of the shuffling process is fixed,\n",
    "#it is possible to directly load the files\n",
    "\n",
    "try:\n",
    "    with open(\"./elections_loglikelihood\", \"rb\") as fp:   # Unpickling\n",
    "        loglikelihood = pickle.load(fp)\n",
    "    with open(\"./elections_logprior\", \"rb\") as fp:   # Unpickling\n",
    "        logprior = pickle.load(fp)\n",
    "except IOError:\n",
    "    logprior, loglikelihood = elections_utils.train_naive_bayes(freqs_dict, x_train, y_train)\n",
    "    with open(\"elections_logprior\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(logprior, fp)\n",
    "    with open(\"elections_loglikelihood\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(loglikelihood, fp)\n",
    "#correctly, the log prior is greater than zero\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the next cell and try to see if some words that you think characterize a candidate have the very high (or very low, if negative) loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.296585836331416\n",
      "-5.234388760385825\n"
     ]
    }
   ],
   "source": [
    "print(elections_utils.naive_bayes_predict('MAGA God golf',logprior,loglikelihood))\n",
    "print(elections_utils.naive_bayes_predict('hunter heal Delaware',logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.7113\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (elections_utils.test_naive_bayes(x_test, y_test, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vetsforsci', 6.494774067358912), ('fucksshit', 6.447026304485696), ('‚öú', 5.838958133164327), ('operationmaga', 5.805301110160851), ('fs', 5.804077868418107), ('negligenthomicid', 5.8028531285222105), ('makeamericaunitedagain', 5.398612760309942), ('murdererinchief', 5.39493403145248), ('politicalview', 4.990082985383875), ('but-add', 4.973369504410134), ('racehorsetheori', 4.671859430351061), ('agonist', 4.627867861069527), ('arizonacircl', 4.426314942347685), ('kanganaranaut', 4.416558767402321), ('kiddwaya', 4.416558767402321), ('azkar', 4.416558767402321), ('eath', 4.355934145585886), ('eugen', 4.3133745311670895), ('unscientif', 4.285824579398859), ('=D', 4.179843138542567)]\n",
      "[('mikeespym', -4.413106665272917), ('wedidntstartthefir', -4.44252055047921), ('re-shar', -4.44252055047921), ('billyl', -4.4569092879313095), ('demcast', -4.46503941401456), (\"crazyuncledonnie'\", -4.4710939229232665), ('demcastpa', -4.4710939229232665), ('follo', -4.602863200554389), ('4follo', -4.602863200554389), ('ditchmoscowmitch', -4.611059967758568), ('middle-eastern', -4.772762237349787), ('crony-stack', -4.813584231870043), ('believeinamerica', -5.044695952833429), ('workload', -5.150056468491255), ('teamjustic', -5.183767525833567), ('wtp', -5.1949517883991465), ('stopselectiontheft', -5.455438118042437), ('issick', -5.471104234786837), ('wtpblue', -6.6450984596579294), ('wtpsenat', -7.656165325929859)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(loglikelihood).most_common()[:20])\n",
    "print(Counter(loglikelihood).most_common()[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the accuracy is not particularly high. In part 3 we will try to divide the datasets into two sub-datasets each, taking into account the sentiment of the tweet. The reason for this is that tweets about both politicians may talk about the same topics, but with opposite sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Sentiment analysis of Trump-Biden datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part, we will take a pre-trained Naive Bayes model for sentiment analysis and evaluate the \"average positiveness\" of the two datasets of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to ./...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9968\n"
     ]
    }
   ],
   "source": [
    "#names are swapped (train_x instead of x_train) in order to not override the previous variables\n",
    "nltk.download('twitter_samples',download_dir=\"./\")\n",
    "\n",
    "all_positive_tweets = []\n",
    "all_negative_tweets = []\n",
    "for line in open('./corpora/twitter_samples/positive_tweets.json', 'r'):\n",
    "    all_positive_tweets.append(json.loads(line)['text'])\n",
    "for line in open('./corpora/twitter_samples/negative_tweets.json', 'r'):\n",
    "    all_negative_tweets.append(json.loads(line)['text'])\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[1000:]\n",
    "train_pos = all_positive_tweets[:7000]\n",
    "test_neg = all_negative_tweets[1000:]\n",
    "train_neg = all_negative_tweets[:7000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "\n",
    "try:\n",
    "    with open(\"./sample_loglikelihood\", \"rb\") as fp:   # Unpickling\n",
    "        sample_loglikelihood = pickle.load(fp)\n",
    "    with open(\"./sample_logprior\", \"rb\") as fp:   # Unpickling\n",
    "        sample_logprior = pickle.load(fp)\n",
    "#    with open(\"./sample_freqs\", \"rb\") as fp:\n",
    "#        sample_freqs = pickle.load(fp)\n",
    "except IOError:       \n",
    "    #names are swapped (train_x instead of x_train) in order to not override the previous variables\n",
    "    print('evaluating freqs')\n",
    "    sample_freqs = elections_utils.count_tweets({}, train_x, train_y)\n",
    "    print('evaluating prior and likelihood')\n",
    "    sample_logprior, sample_loglikelihood = elections_utils.train_naive_bayes(sample_freqs, train_x, train_y)\n",
    "    with open(\"sample_logprior\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(sample_logprior, fp)\n",
    "    with open(\"sample_loglikelihood\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(sample_loglikelihood, fp)\n",
    "print(\"Naive Bayes accuracy = %0.4f\" %(elections_utils.test_naive_bayes(test_x, test_y, sample_logprior, sample_loglikelihood)))\n",
    "#the accuracy is >99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average positiveness (aka log posterior) of tweets about Trump: -0.3546\n",
      "Average positiveness (aka log posterior) of tweets about Biden: -0.1246\n"
     ]
    }
   ],
   "source": [
    "log_posterior_trump=elections_utils.average_log_posterior(trump_tweets,sample_logprior, sample_loglikelihood)\n",
    "log_posterior_biden=elections_utils.average_log_posterior(biden_tweets,sample_logprior, sample_loglikelihood)\n",
    "print(\"Average positiveness (aka log posterior) of tweets about Trump: %0.4f\"\n",
    "      %(log_posterior_trump))\n",
    "print(\"Average positiveness (aka log posterior) of tweets about Biden: %0.4f\"\n",
    "      %(log_posterior_biden))\n",
    "#notice that the average of the log posterior is much different from the average of the posterior, and in general\n",
    "#there is no link between the two, except that since the logarithmic and exponential functions are both monothonous,\n",
    "#a higher logposterior implies a higher posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the logs of posteriors makes quantifying how much more positive Biden tweets are quite difficult. If that was the goal of the project, using non-logarithmic posteriors would have been better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Sentiment analysis to filter tweets, and then Trump-Biden classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_trump_tweets, neg_trump_tweets=elections_utils.divide_pos_neg(trump_tweets, [],[],sample_logprior, sample_loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_biden_tweets, neg_biden_tweets=elections_utils.divide_pos_neg(biden_tweets, [],[],sample_logprior, sample_loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162369\n",
      "67942\n",
      "86941\n",
      "138367\n",
      "61565\n",
      "69768\n"
     ]
    }
   ],
   "source": [
    "print(len(trump_tweets))\n",
    "print(len(pos_trump_tweets))\n",
    "print(len(neg_trump_tweets))\n",
    "print(len(biden_tweets))\n",
    "print(len(pos_biden_tweets))\n",
    "print(len(neg_biden_tweets))\n",
    "#should be not surprised that the sum of pos+neg does not give the total, because some tweets contain only words \n",
    "#that are not in the sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.A: Analysis of positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_trump_train=pos_trump_tweets[:-round(len(pos_trump_tweets)/10)]\n",
    "pos_trump_test=pos_trump_tweets[-round(len(pos_trump_tweets)/10):]\n",
    "pos_biden_train=pos_biden_tweets[:-round(len(pos_biden_tweets)/10)]\n",
    "pos_biden_test=pos_biden_tweets[-round(len(pos_biden_tweets)/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x_train=pos_trump_train+pos_biden_train\n",
    "pos_x_test=pos_trump_test+pos_biden_test\n",
    "pos_y_train=np.append(np.ones(len(pos_trump_train)), np.zeros(len(pos_biden_train)))\n",
    "pos_y_test=np.append(np.ones(len(pos_trump_test)), np.zeros(len(pos_biden_test)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"./elections_pos_dictionary\", \"rb\") as fp:   # Unpickling\n",
    "        pos_freqs_dict = pickle.load(fp)\n",
    "except IOError:\n",
    "    pos_freqs_dict = elections_utils.count_tweets({}, pos_x_train, pos_y_train)\n",
    "    with open(\"elections_pos_dictionary\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(pos_freqs_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"./elections_pos_loglikelihood\", \"rb\") as fp:   # Unpickling\n",
    "        pos_loglikelihood = pickle.load(fp)\n",
    "    with open(\"./elections_pos_logprior\", \"rb\") as fp:   # Unpickling\n",
    "        pos_logprior = pickle.load(fp)\n",
    "except IOError:\n",
    "    pos_logprior, pos_loglikelihood = elections_utils.train_naive_bayes(pos_freqs_dict, pos_x_train, pos_y_train)\n",
    "    with open(\"elections_pos_logprior\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(pos_logprior, fp)\n",
    "    with open(\"elections_pos_loglikelihood\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(pos_loglikelihood, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.7120\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (elections_utils.test_naive_bayes(pos_x_test, pos_y_test, pos_logprior, pos_loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('agonist', 4.685828829476688), ('rememberinnovemb', 4.5625306131321945), ('funfact', 4.362864942880268), ('politicalview', 4.226962944641409), ('refusestolead', 4.226962944641409), ('professionaladvic', 4.226962944641409), ('democratichoax', 4.226962944641409), ('happytalk', 4.226962944641409), ('didnthavetobethisway', 4.226962944641409), ('eath', 4.134181211190443)]\n",
      "[('follo', -4.459973021361923), ('4follo', -4.459973021361923), ('üõç', -4.631121277557752), ('middle-eastern', -4.696361799426153), ('stopselectiontheft', -4.767457721109883), ('demcast', -4.887417036188863), ('teamjustic', -5.579113890171161), ('wtp', -5.772501232242204), ('wtpblue', -6.4278620771360115), ('wtpsenat', -7.467633865781187)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pos_loglikelihood).most_common()[:10])\n",
    "print(Counter(pos_loglikelihood).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.B: Analysis of negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_trump_train=neg_trump_tweets[:-round(len(neg_trump_tweets)/10)]\n",
    "neg_trump_test=neg_trump_tweets[-round(len(neg_trump_tweets)/10):]\n",
    "neg_biden_train=neg_biden_tweets[:-round(len(neg_biden_tweets)/10)]\n",
    "neg_biden_test=neg_biden_tweets[-round(len(neg_biden_tweets)/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_x_train=neg_trump_train+neg_biden_train\n",
    "neg_x_test=neg_trump_test+neg_biden_test\n",
    "neg_y_train=np.append(np.ones(len(neg_trump_train)), np.zeros(len(neg_biden_train)))\n",
    "neg_y_test=np.append(np.ones(len(neg_trump_test)), np.zeros(len(neg_biden_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"./elections_neg_dictionary\", \"rb\") as fp:   # Unpickling\n",
    "        neg_freqs_dict = pickle.load(fp)\n",
    "except IOError:\n",
    "    neg_freqs_dict = elections_utils.count_tweets({}, neg_x_train, neg_y_train)\n",
    "    with open(\"elections_neg_dictionary\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(neg_freqs_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"./elections_neg_loglikelihood\", \"rb\") as fp:   # Unpickling\n",
    "        neg_loglikelihood = pickle.load(fp)\n",
    "    with open(\"./elections_neg_logprior\", \"rb\") as fp:   # Unpickling\n",
    "        neg_logprior = pickle.load(fp)\n",
    "except IOError:\n",
    "    neg_logprior, neg_loglikelihood = elections_utils.train_naive_bayes(neg_freqs_dict, neg_x_train, neg_y_train)\n",
    "    with open(\"elections_neg_logprior\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(neg_logprior, fp)\n",
    "    with open(\"elections_neg_loglikelihood\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(neg_loglikelihood, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.7038\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (elections_utils.test_naive_bayes(neg_x_test, neg_y_test, neg_logprior, neg_loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vetsforsci', 6.430334047507442), ('fucksshit', 6.398625620622301), ('‚öú', 5.784727811716496), ('negligenthomicid', 5.739680633825898), ('fs', 5.738434527745651), ('makeamericaunitedagain', 5.737186866947496), ('operationmaga', 5.737186866947496), ('murdererinchief', 5.729668034533469), ('‚ú≥', 5.216743774110557), ('prone', 4.959408858048068)]\n",
      "[('üîÅ', -4.573015013097228), ('ditchmoscowmitch', -4.658172821437535), ('stopselectiontheft', -4.7441632692930575), ('crony-stack', -4.860697085549009), ('cumul', -5.075808465165955), ('believeinamerica', -5.07983261546568), ('workload', -5.182780584718122), ('issick', -5.298952016480165), ('wtpsenat', -5.459533586637713), ('wtpblue', -5.827680931738683)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(neg_loglikelihood).most_common()[:10])\n",
    "print(Counter(neg_loglikelihood).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, the accuracy didn't improve much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
